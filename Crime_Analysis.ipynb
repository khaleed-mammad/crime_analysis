{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644b9867",
   "metadata": {},
   "source": [
    "## Uploading the data to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142ffec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: hdfs in /opt/anaconda3/lib/python3.11/site-packages (2.7.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: docopt in /opt/anaconda3/lib/python3.11/site-packages (from hdfs) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from hdfs) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b9f313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch and upload process...\n",
      "\n",
      "Fetching data: Offset=0, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_1.csv (Total rows fetched: 50000)\n",
      "\n",
      "Fetching data: Offset=50000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_2.csv (Total rows fetched: 100000)\n",
      "\n",
      "Fetching data: Offset=100000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_3.csv (Total rows fetched: 150000)\n",
      "\n",
      "Fetching data: Offset=150000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_4.csv (Total rows fetched: 200000)\n",
      "\n",
      "Fetching data: Offset=200000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_5.csv (Total rows fetched: 250000)\n",
      "\n",
      "Fetching data: Offset=250000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_6.csv (Total rows fetched: 300000)\n",
      "\n",
      "Fetching data: Offset=300000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_7.csv (Total rows fetched: 350000)\n",
      "\n",
      "Fetching data: Offset=350000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_8.csv (Total rows fetched: 400000)\n",
      "\n",
      "Fetching data: Offset=400000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_9.csv (Total rows fetched: 450000)\n",
      "\n",
      "Fetching data: Offset=450000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_10.csv (Total rows fetched: 500000)\n",
      "\n",
      "Fetching data: Offset=500000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_11.csv (Total rows fetched: 550000)\n",
      "\n",
      "Fetching data: Offset=550000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_12.csv (Total rows fetched: 600000)\n",
      "\n",
      "Fetching data: Offset=600000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_13.csv (Total rows fetched: 650000)\n",
      "\n",
      "Fetching data: Offset=650000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_14.csv (Total rows fetched: 700000)\n",
      "\n",
      "Fetching data: Offset=700000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_15.csv (Total rows fetched: 750000)\n",
      "\n",
      "Fetching data: Offset=750000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_16.csv (Total rows fetched: 800000)\n",
      "\n",
      "Fetching data: Offset=800000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_17.csv (Total rows fetched: 850000)\n",
      "\n",
      "Fetching data: Offset=850000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_18.csv (Total rows fetched: 900000)\n",
      "\n",
      "Fetching data: Offset=900000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_19.csv (Total rows fetched: 950000)\n",
      "\n",
      "Fetching data: Offset=950000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_20.csv (Total rows fetched: 1000000)\n",
      "\n",
      "Fetching data: Offset=1000000, Limit=50000 (Attempt 1)...\n",
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_21.csv (Total rows fetched: 1050000)\n",
      "\n",
      "Fetching data: Offset=1050000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_22.csv (Total rows fetched: 1100000)\n",
      "\n",
      "Fetching data: Offset=1100000, Limit=50000 (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_62806/2747920531.py:45: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  chunk = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk to HDFS: /input/nypd_data/NYPD_Complaint_Data_Chunk_23.csv (Total rows fetched: 1150000)\n",
      "\n",
      "Process complete. Total rows fetched: 1150000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# HDFS client configuration\n",
    "hdfs_client = InsecureClient('http://localhost:9870', user='khaleed_mammad')\n",
    "hdfs_path = '/input/nypd_data/'\n",
    "\n",
    "# API endpoint\n",
    "api_url = \"https://data.cityofnewyork.us/resource/qgea-i56i.csv\"\n",
    "\n",
    "# Parameters for pagination\n",
    "limit = 50000  # Maximum rows per request\n",
    "offset = 0\n",
    "rows_to_fetch = 8900000 // 8  \n",
    "rows_fetched = 0\n",
    "\n",
    "\n",
    "max_retries = 5\n",
    "retry_delay = 5\n",
    "\n",
    "chunk_size = 100000  # Rows per chunk\n",
    "\n",
    "print(\"Starting data fetch and upload process...\\n\")\n",
    "\n",
    "# Fetch and upload the dataset in chunks\n",
    "while rows_fetched < rows_to_fetch:\n",
    "    params = {\"$limit\": limit, \"$offset\": offset}\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Fetching data: Offset={offset}, Limit={limit} (Attempt {retries + 1})...\")\n",
    "            response = requests.get(api_url, params=params, timeout=30)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "                retries += 1\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "\n",
    "            # Read the chunk into a DataFrame\n",
    "            chunk = pd.read_csv(StringIO(response.text))\n",
    "            if chunk.empty:\n",
    "                print(\"No more data to fetch. Exiting.\")\n",
    "                break\n",
    "\n",
    "            rows_fetched += len(chunk)\n",
    "            offset += limit\n",
    "\n",
    "            # Upload the chunk to HDFS\n",
    "            chunk_path = f'/input/nypd_data/NYPD_Complaint_Data_Chunk_{offset//limit}.csv'\n",
    "            with StringIO() as csv_buffer:\n",
    "                chunk.to_csv(csv_buffer, index=False)\n",
    "                hdfs_client.write(chunk_path, csv_buffer.getvalue(), overwrite=True)\n",
    "            print(f\"Uploaded chunk to HDFS: {chunk_path} (Total rows fetched: {rows_fetched})\\n\")\n",
    "            break\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Request timed out. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    if retries == max_retries:\n",
    "        print(f\"Max retries reached for Offset={offset}. Exiting.\")\n",
    "        break\n",
    "\n",
    "print(f\"Process complete. Total rows fetched: {rows_fetched}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c98fe",
   "metadata": {},
   "source": [
    "## Reading data and analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf18a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import to_date \n",
    "from pyspark.sql.functions import year, month \n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8504079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 CSV files. Merging now...\n",
      "Reading file: NYPD_Complaint_Data_Chunk_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_11.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_13.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_14.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_4.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_7.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_1.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_19.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/v1sknhls4s34d0r6p0v084fh0000gn/T/ipykernel_66434/2404809267.py:27: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: NYPD_Complaint_Data_Chunk_20.csv\n",
      "Reading file: NYPD_Complaint_Data_Chunk_21.csv\n",
      "All files merged successfully into: /Users/khaleed_mammad/Desktop/Khaleed/ADA_Semesters/Fall_2024/Big_Data/Project/NYPD_Complaint_Data_Combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "directory = '/Users/khaleed_mammad/Desktop/Khaleed/ADA_Semesters/Fall_2024/Big_Data/Project'\n",
    "\n",
    "# Output file path\n",
    "output_file = os.path.join(directory, 'NYPD_Complaint_Data_Combined.csv')\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Check if there are CSV files\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Merging now...\")\n",
    "\n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through each CSV file and append its DataFrame to the list\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        print(f\"Reading file: {file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"All files merged successfully into: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "222bd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYPD Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the local combined CSV file\n",
    "local_csv_path = output_file\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(local_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73884939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+------------+-----------+-------------------+-----+--------------------+-----+--------------------+----------------+-----------+-------------+-----------------+--------------------+-------------------+-----------------+--------+----------+-----------+----------+----------+--------------+--------------+--------+----------------+---------+----------+--------------------+--------------------+------------+-------------+--------+-------+\n",
      "|cmplnt_num|       cmplnt_fr_dt|       cmplnt_fr_tm|       cmplnt_to_dt|cmplnt_to_tm|addr_pct_cd|             rpt_dt|ky_cd|           ofns_desc|pd_cd|             pd_desc|crm_atpt_cptd_cd| law_cat_cd|      boro_nm|loc_of_occur_desc|       prem_typ_desc|         juris_desc|jurisdiction_code|parks_nm|hadevelopt|housing_psa|x_coord_cd|y_coord_cd|susp_age_group|     susp_race|susp_sex|transit_district| latitude| longitude|             lat_lon|         patrol_boro|station_name|vic_age_group|vic_race|vic_sex|\n",
      "+----------+-------------------+-------------------+-------------------+------------+-----------+-------------------+-----+--------------------+-----+--------------------+----------------+-----------+-------------+-----------------+--------------------+-------------------+-----------------+--------+----------+-----------+----------+----------+--------------+--------------+--------+----------------+---------+----------+--------------------+--------------------+------------+-------------+--------+-------+\n",
      "| 265088253|2023-02-09 00:00:00|2024-11-22 15:00:00|2023-03-09 00:00:00|    15:00:00|       26.0|2023-03-14 00:00:00|  109|       GRAND LARCENY|439.0|LARCENY,GRAND FRO...|       COMPLETED|     FELONY|    MANHATTAN|         FRONT OF|      MAILBOX INSIDE|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  995804.0|  235548.0|       UNKNOWN|       UNKNOWN|       U|            NULL|40.813196|-73.958257|(40.813196, -73.9...|PATROL BORO MAN N...|      (null)|          65+|   WHITE|      M|\n",
      "| 265110558|2023-03-14 00:00:00|2024-11-22 21:12:00|2023-03-14 00:00:00|    21:12:00|       28.0|2023-03-14 00:00:00|  113|             FORGERY|729.0|FORGERY,ETC.,UNCL...|       COMPLETED|     FELONY|    MANHATTAN|         FRONT OF|              STREET|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  997571.0|  234556.0|         25-44|WHITE HISPANIC|       M|            NULL|40.810469|-73.951878|(40.810469, -73.9...|PATROL BORO MAN N...|      (null)|      UNKNOWN| UNKNOWN|      E|\n",
      "| 265096674|2023-03-14 00:00:00|2024-11-22 16:45:00|2023-03-14 00:00:00|    16:45:00|      122.0|2023-03-14 00:00:00|  118|   DANGEROUS WEAPONS|792.0|CRIMINAL POSSESSI...|       COMPLETED|     FELONY|STATEN ISLAND|         FRONT OF|RESIDENCE - PUBLI...|N.Y. HOUSING POLICE|                2|  (null)|    (null)|       1291|  962198.0|  155911.0|         25-44|         BLACK|       M|            NULL|40.594591|-74.079403|(40.594591, -74.0...|PATROL BORO STATE...|      (null)|      UNKNOWN| UNKNOWN|      E|\n",
      "| 265104725|2023-03-14 00:00:00|2024-11-22 17:30:00|2023-03-14 00:00:00|    17:57:00|        9.0|2023-03-14 00:00:00|  361|OFF. AGNST PUB OR...|661.0|     LEWDNESS,PUBLIC|       COMPLETED|MISDEMEANOR|    MANHATTAN|           INSIDE|    RESTAURANT/DINER|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  986428.0|  205224.0|         25-44|         BLACK|       M|            NULL|40.729971|-73.992138|(40.729971, -73.9...|PATROL BORO MAN S...|      (null)|      UNKNOWN| UNKNOWN|      F|\n",
      "| 265096568|2023-03-14 00:00:00|2024-11-22 12:00:00|2023-03-14 00:00:00|    12:10:00|        6.0|2023-03-14 00:00:00|  112|         THEFT-FRAUD|739.0|FRAUD,UNCLASSIFIE...|       COMPLETED|     FELONY|    MANHATTAN|           INSIDE|RESIDENCE - APT. ...|   N.Y. POLICE DEPT|                0|  (null)|    (null)|     (null)|  983857.0|  205980.0|       UNKNOWN|       UNKNOWN|       U|            NULL|40.732047|-74.001415|(40.732047, -74.0...|PATROL BORO MAN S...|      (null)|        45-64|   WHITE|      M|\n",
      "+----------+-------------------+-------------------+-------------------+------------+-----------+-------------------+-----+--------------------+-----+--------------------+----------------+-----------+-------------+-----------------+--------------------+-------------------+-----------------+--------+----------+-----------+----------+----------+--------------+--------------+--------+----------------+---------+----------+--------------------+--------------------+------------+-------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de404f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cmplnt_num='265088253', cmplnt_fr_dt=datetime.datetime(2023, 2, 9, 0, 0), cmplnt_fr_tm=datetime.datetime(2024, 11, 22, 15, 0), cmplnt_to_dt=datetime.datetime(2023, 3, 9, 0, 0), cmplnt_to_tm='15:00:00', addr_pct_cd=26.0, rpt_dt=datetime.datetime(2023, 3, 14, 0, 0), ky_cd=109, ofns_desc='GRAND LARCENY', pd_cd=439.0, pd_desc='LARCENY,GRAND FROM OPEN AREAS, UNATTENDED', crm_atpt_cptd_cd='COMPLETED', law_cat_cd='FELONY', boro_nm='MANHATTAN', loc_of_occur_desc='FRONT OF', prem_typ_desc='MAILBOX INSIDE', juris_desc='N.Y. POLICE DEPT', jurisdiction_code=0, parks_nm='(null)', hadevelopt='(null)', housing_psa='(null)', x_coord_cd=995804.0, y_coord_cd=235548.0, susp_age_group='UNKNOWN', susp_race='UNKNOWN', susp_sex='U', transit_district=None, latitude=40.813196, longitude=-73.958257, lat_lon='(40.813196, -73.958257)', patrol_boro='PATROL BORO MAN NORTH', station_name='(null)', vic_age_group='65+', vic_race='WHITE', vic_sex='M')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e3557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cmplnt_num: string (nullable = true)\n",
      " |-- cmplnt_fr_dt: timestamp (nullable = true)\n",
      " |-- cmplnt_fr_tm: timestamp (nullable = true)\n",
      " |-- cmplnt_to_dt: timestamp (nullable = true)\n",
      " |-- cmplnt_to_tm: string (nullable = true)\n",
      " |-- addr_pct_cd: double (nullable = true)\n",
      " |-- rpt_dt: timestamp (nullable = true)\n",
      " |-- ky_cd: integer (nullable = true)\n",
      " |-- ofns_desc: string (nullable = true)\n",
      " |-- pd_cd: double (nullable = true)\n",
      " |-- pd_desc: string (nullable = true)\n",
      " |-- crm_atpt_cptd_cd: string (nullable = true)\n",
      " |-- law_cat_cd: string (nullable = true)\n",
      " |-- boro_nm: string (nullable = true)\n",
      " |-- loc_of_occur_desc: string (nullable = true)\n",
      " |-- prem_typ_desc: string (nullable = true)\n",
      " |-- juris_desc: string (nullable = true)\n",
      " |-- jurisdiction_code: integer (nullable = true)\n",
      " |-- parks_nm: string (nullable = true)\n",
      " |-- hadevelopt: string (nullable = true)\n",
      " |-- housing_psa: string (nullable = true)\n",
      " |-- x_coord_cd: double (nullable = true)\n",
      " |-- y_coord_cd: double (nullable = true)\n",
      " |-- susp_age_group: string (nullable = true)\n",
      " |-- susp_race: string (nullable = true)\n",
      " |-- susp_sex: string (nullable = true)\n",
      " |-- transit_district: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lat_lon: string (nullable = true)\n",
      " |-- patrol_boro: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- vic_age_group: string (nullable = true)\n",
      " |-- vic_race: string (nullable = true)\n",
      " |-- vic_sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0644c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, cmplnt_num: string, cmplnt_to_tm: string, addr_pct_cd: string, ky_cd: string, ofns_desc: string, pd_cd: string, pd_desc: string, crm_atpt_cptd_cd: string, law_cat_cd: string, boro_nm: string, loc_of_occur_desc: string, prem_typ_desc: string, juris_desc: string, jurisdiction_code: string, parks_nm: string, hadevelopt: string, housing_psa: string, x_coord_cd: string, y_coord_cd: string, susp_age_group: string, susp_race: string, susp_sex: string, transit_district: string, latitude: string, longitude: string, lat_lon: string, patrol_boro: string, station_name: string, vic_age_group: string, vic_race: string, vic_sex: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d626b801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmplnt_num',\n",
       " 'cmplnt_fr_dt',\n",
       " 'cmplnt_fr_tm',\n",
       " 'cmplnt_to_dt',\n",
       " 'cmplnt_to_tm',\n",
       " 'addr_pct_cd',\n",
       " 'rpt_dt',\n",
       " 'ky_cd',\n",
       " 'ofns_desc',\n",
       " 'pd_cd',\n",
       " 'pd_desc',\n",
       " 'crm_atpt_cptd_cd',\n",
       " 'law_cat_cd',\n",
       " 'boro_nm',\n",
       " 'loc_of_occur_desc',\n",
       " 'prem_typ_desc',\n",
       " 'juris_desc',\n",
       " 'jurisdiction_code',\n",
       " 'parks_nm',\n",
       " 'hadevelopt',\n",
       " 'housing_psa',\n",
       " 'x_coord_cd',\n",
       " 'y_coord_cd',\n",
       " 'susp_age_group',\n",
       " 'susp_race',\n",
       " 'susp_sex',\n",
       " 'transit_district',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'lat_lon',\n",
       " 'patrol_boro',\n",
       " 'station_name',\n",
       " 'vic_age_group',\n",
       " 'vic_race',\n",
       " 'vic_sex']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea75e83",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f869e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       cmplnt_fr_dt|\n",
      "+-------------------+\n",
      "|2023-02-09 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "|2023-03-14 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cmplnt_fr_dt\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bf47785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we create columns to store 'Year, month, hour and day of the week' of start date of complaint\n",
    "df = df.withColumn('Start_Date', to_timestamp('cmplnt_fr_dt', 'MM/dd/yyyy'))\n",
    "df = df.withColumn('Date', to_date('Start_Date'))\n",
    "df = df.withColumn('Hour', hour(df['cmplnt_fr_tm']))\n",
    "df = df.withColumn('YEAR', year('Date')) \n",
    "df = df.withColumn('MONTH', month('Date'))\n",
    "df = df.withColumn(\"DAY_of_WEEK\", date_format(\"Date\", \"E\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d89f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|vic_age_group| count|\n",
      "+-------------+------+\n",
      "|          <18| 43353|\n",
      "|        25-44|409647|\n",
      "|      UNKNOWN|326170|\n",
      "|         1018|     2|\n",
      "|          -49|     1|\n",
      "|          949|     1|\n",
      "|          65+| 58291|\n",
      "|         -968|     1|\n",
      "|        18-24| 95680|\n",
      "|          -33|     1|\n",
      "|           -3|     2|\n",
      "|        45-64|216793|\n",
      "|           -2|     3|\n",
      "|          -30|     3|\n",
      "|           -6|     2|\n",
      "|          -57|     1|\n",
      "|          950|     1|\n",
      "|         -945|     1|\n",
      "|         -963|     1|\n",
      "|          -27|     1|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('vic_age_group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb86946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can categorize age groups --> Teenager, Young Adult, Middle Age, Mid Old\n",
    "df1 = df1.withColumn('VIC_AGE_Cat',\n",
    "    when(df1.VIC_AGE_GROUP == '<18', 'Teenager')\n",
    "    .when(df1.VIC_AGE_GROUP == '18-25', 'Young Adult')\n",
    "    .when(df1.VIC_AGE_GROUP == '25-44', 'Middle Age')\n",
    "    .when(df1.VIC_AGE_GROUP == '44-64', 'Mid Old')\n",
    "    .otherwise('Senior'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2815adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# We can categorize age groups --> Teenager, Young Adult, Middle Age, Mid Old and Senior\n",
    "\n",
    "# As UNKNOWN occurs 326170 times, we replace it with mode instead of instead of deleting (but we delete invalid ages)\n",
    "\n",
    "mode_value = (\n",
    "    df.filter(df[\"vic_age_group\"] != \"UNKNOWN\")\n",
    "    .groupBy(\"vic_age_group\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .first()[\"vic_age_group\"]  # Get the value of the most frequent category\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"vic_age_group\",\n",
    "    when(df[\"vic_age_group\"] == \"UNKNOWN\", mode_value)  # Replace UNKNOWN with mode\n",
    "    .when(df[\"vic_age_group\"].isin(\"<18\", \"18-24\", \"25-44\", \"45-64\", \"65+\"), df[\"vic_age_group\"])  # Keep valid categories\n",
    "    .otherwise(None)  # Mark invalid values as NULL for removal\n",
    ")\n",
    "\n",
    "df = df.filter(df[\"vic_age_group\"].isNotNull())\n",
    "\n",
    "# Now, we can categorize\n",
    "df = df.withColumn(\n",
    "    \"vic_age_category\",\n",
    "    when(df[\"vic_age_group\"] == \"<18\", \"Teenager\")\n",
    "    .when(df[\"vic_age_group\"] == \"18-24\", \"Young Adult\")\n",
    "    .when(df[\"vic_age_group\"] == \"25-44\", \"Middle Age\")\n",
    "    .when(df[\"vic_age_group\"] == \"45-64\", \"Mid Old\")\n",
    "    .when(df[\"vic_age_group\"] == \"65+\", \"Senior\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "811808c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|vic_age_group| count|\n",
      "+-------------+------+\n",
      "|          <18| 43353|\n",
      "|        25-44|735817|\n",
      "|          65+| 58291|\n",
      "|        18-24| 95680|\n",
      "|        45-64|216793|\n",
      "+-------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('vic_age_group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50877071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 37:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|vic_age_category| count|\n",
      "+----------------+------+\n",
      "|          Senior| 58291|\n",
      "|        Teenager| 43353|\n",
      "|      Middle Age|735817|\n",
      "|     Young Adult| 95680|\n",
      "|         Mid Old|216793|\n",
      "+----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('vic_age_category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c176f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to drop some irrelevant and redundant columns (some are too specific and some of them have too many null values)\n",
    "dataset = df.drop('cmplnt_num','cmplnt_fr_dt', 'cmplnt_fr_tm', 'cmplnt_to_dt', 'cmplnt_to_tm','pd_cd','rpt_dt',\n",
    "                 'pd_desc','loc_of_occur_desc', 'prem_typ_desc', 'juris_desc', 'jurisdiction_code', 'parks_nm',\n",
    "                  'hadevelopt', 'housing_psa','x_coord_cd','y_coord_cd','susp_age_group','susp_race', \n",
    "                  'susp_sex', 'transit_district','patrol_boro','station_name','vic_age_group','vic_race', 'Start_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f8f0cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+----------------+----------+---------+---------+----------+--------------------+-------+----------+----+----+-----+-----------+----------------+\n",
      "|addr_pct_cd|ky_cd|    ofns_desc|crm_atpt_cptd_cd|law_cat_cd|  boro_nm| latitude| longitude|             lat_lon|vic_sex|      Date|Hour|YEAR|MONTH|DAY_of_WEEK|vic_age_category|\n",
      "+-----------+-----+-------------+----------------+----------+---------+---------+----------+--------------------+-------+----------+----+----+-----+-----------+----------------+\n",
      "|       26.0|  109|GRAND LARCENY|       COMPLETED|    FELONY|MANHATTAN|40.813196|-73.958257|(40.813196, -73.9...|      M|2023-02-09|  15|2023|    2|        Thu|          Senior|\n",
      "+-----------+-----+-------------+----------------+----------+---------+---------+----------+--------------------+-------+----------+----+----+-----+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "710a0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the columns for better readabiliy\n",
    "\n",
    "dataframe = dataset.toDF('Neighborhood', 'Offence_Code', 'Offence_Type',\n",
    "        'Status','Offence_Level','Borough','Latitude','Longitude','Lat_Lon', 'Victim_Sex', 'Date', 'Hour', 'Year', 'Month', 'Day_of_Week','Victim_Agegroup')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48ca4973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We also handle with null values\n",
    "\n",
    "mean_long = dataframe.agg({'Longitude': 'mean'}).collect()[0][0] \n",
    "dataframe = dataframe.fillna(mean_long, subset=['Longitude'])\n",
    "\n",
    "mean_lat = dataframe.agg({'Latitude': 'mean'}).collect()[0][0] \n",
    "dataframe = dataframe.fillna(mean_lat, subset=['Latitude'])\n",
    "\n",
    "dataframe = dataframe.fillna(-1, subset=['Borough','Neighborhood','Offence_Code','Offence_Type','Status','Offence_level','Victim_Sex'])\n",
    "dataframe = dataframe.na.drop('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d958e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous dataset: 1149934 rows \n",
      "New dataset: 1149919 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:=====================>                                    (3 + 5) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Previous dataset: {df.count()} rows \\nNew dataset: {dataframe.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a6d54167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We save the dataset\n",
    "dataframe.write.csv(\"cleaned_nypd_data.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb769317",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32707091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/24 17:38:15 WARN Utils: Your hostname, Khaleeds-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.31.31 instead (on interface en0)\n",
      "24/11/24 17:38:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/24 17:38:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYPD Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8f2492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_data = spark.read.csv(\"cleaned_nypd_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d036103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database nypddatabase\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db5d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")\n",
    "\n",
    "cleaned_data.write.mode(\"overwrite\").saveAsTable(\"nypddatabase.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99efce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|      Borough| Count|\n",
      "+-------------+------+\n",
      "|     BROOKLYN|319820|\n",
      "|    MANHATTAN|280934|\n",
      "|       QUEENS|251879|\n",
      "|        BRONX|244984|\n",
      "|STATEN ISLAND| 50043|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the SQL query\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT Borough, COUNT(*) AS Count\n",
    "    FROM nypddatabase.df\n",
    "    GROUP BY Borough\n",
    "    ORDER BY Count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff1c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|Offence_Level| Count|\n",
      "+-------------+------+\n",
      "|       FELONY|388294|\n",
      "|  MISDEMEANOR|581826|\n",
      "|    VIOLATION|179799|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Level, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Level\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e30d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|Offence_Level|Victim_Sex| Count|\n",
      "+-------------+----------+------+\n",
      "|  MISDEMEANOR|         D|138826|\n",
      "|  MISDEMEANOR|         M|180814|\n",
      "|       FELONY|         M|154413|\n",
      "|       FELONY|         L|  4248|\n",
      "|       FELONY|         D| 55350|\n",
      "|    VIOLATION|         D|  1650|\n",
      "|       FELONY|         F|133428|\n",
      "|  MISDEMEANOR|         L|  1613|\n",
      "|    VIOLATION|         F|112429|\n",
      "|       FELONY|         E| 40854|\n",
      "|    VIOLATION|         L|   197|\n",
      "|  MISDEMEANOR|         F|195483|\n",
      "|  MISDEMEANOR|         E| 65090|\n",
      "|    VIOLATION|         E|  2061|\n",
      "|    VIOLATION|         M| 63462|\n",
      "|       FELONY|         U|     1|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Level,Victim_Sex,  COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Level,Victim_Sex\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79185fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like we have some errors in sex column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67379359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Victim_Sex| Count|\n",
      "+----------+------+\n",
      "|         U|     1|\n",
      "|         L|  6058|\n",
      "|         E|108005|\n",
      "|         D|195826|\n",
      "|         M|398689|\n",
      "|         F|441340|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Victim_Sex,  COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Victim_Sex\n",
    "    ORDER BY Count\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18519fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping U, L, E , D would cause a problem as they contain big protion of our data. Therefore, \n",
    "# we decide to keep it since our purpose is not studying crime patterns by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a925ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+------+\n",
      "|Offence_Level|Year| Count|\n",
      "+-------------+----+------+\n",
      "|    VIOLATION|2021| 10648|\n",
      "|       FELONY|2009|    33|\n",
      "|       FELONY|2022|177146|\n",
      "|       FELONY|1989|     3|\n",
      "|       FELONY|1988|     6|\n",
      "|  MISDEMEANOR|2021| 33976|\n",
      "|       FELONY|1966|     1|\n",
      "|       FELONY|1022|     3|\n",
      "|  MISDEMEANOR|2019|   226|\n",
      "|       FELONY|2021| 27312|\n",
      "|    VIOLATION|1013|     1|\n",
      "|       FELONY|2007|    34|\n",
      "|  MISDEMEANOR|2022|268722|\n",
      "|       FELONY|2012|    85|\n",
      "|       FELONY|1995|     6|\n",
      "|  MISDEMEANOR|2013|    76|\n",
      "|  MISDEMEANOR|1982|     1|\n",
      "|  MISDEMEANOR|1973|     6|\n",
      "|    VIOLATION|2016|     6|\n",
      "|  MISDEMEANOR|1998|     4|\n",
      "+-------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Level,Year,  COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Level,Year\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0815cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        Offence_Type| Count|\n",
      "+--------------------+------+\n",
      "|       PETIT LARCENY|238813|\n",
      "|       HARRASSMENT 2|176697|\n",
      "|ASSAULT 3 & RELAT...|119694|\n",
      "|       GRAND LARCENY|110416|\n",
      "|CRIMINAL MISCHIEF...| 92441|\n",
      "|      FELONY ASSAULT| 56920|\n",
      "|OFF. AGNST PUB OR...| 38688|\n",
      "|VEHICLE AND TRAFF...| 37391|\n",
      "|             ROBBERY| 36442|\n",
      "|MISCELLANEOUS PEN...| 35142|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Offence_Type, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Offence_Type\n",
    "    ORDER BY Count DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b712ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Victim_Agegroup| Count|\n",
      "+---------------+------+\n",
      "|         Senior| 58289|\n",
      "|       Teenager| 43349|\n",
      "|     Middle Age|735812|\n",
      "|    Young Adult| 95679|\n",
      "|        Mid Old|216790|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Victim_Agegroup, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Victim_Agegroup\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ad47974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|Count|\n",
      "+----+-----+\n",
      "|1011|    3|\n",
      "|1012|    6|\n",
      "|1013|    2|\n",
      "|1018|    1|\n",
      "|1021|    2|\n",
      "|1022|   11|\n",
      "|1023|   23|\n",
      "|1900|    1|\n",
      "|1921|    2|\n",
      "|1922|    7|\n",
      "|1923|    5|\n",
      "|1949|    1|\n",
      "|1955|    1|\n",
      "|1961|    1|\n",
      "|1964|    1|\n",
      "|1966|    1|\n",
      "|1967|    1|\n",
      "|1969|    1|\n",
      "|1970|    1|\n",
      "|1971|    2|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Year, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Year\n",
    "    ORDER BY Year\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d262a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like we have errors in year column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc1c36f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Year| Count|\n",
      "+----+------+\n",
      "|2023|543278|\n",
      "|2022|529812|\n",
      "|2021| 71936|\n",
      "|2020|  1547|\n",
      "|2019|   686|\n",
      "|2018|   439|\n",
      "|2017|   304|\n",
      "|2016|   243|\n",
      "|2015|   215|\n",
      "|2014|   186|\n",
      "|2013|   189|\n",
      "|2012|   140|\n",
      "|2011|   132|\n",
      "|2010|    68|\n",
      "|2009|    47|\n",
      "|2008|    41|\n",
      "|2007|    52|\n",
      "|2006|    41|\n",
      "|2005|    28|\n",
      "|2004|    28|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Year, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Year\n",
    "    ORDER BY Year DESC\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e13bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since most of data smaples belong to the years \"2023\", \"2022\", \"2021\" and \"2020\", we only keep those.\n",
    "cleaned_data = cleaned_data.filter(cleaned_data[\"Year\"].isin(2023, 2022, 2021, 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "118eab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Month| Count|\n",
      "+-----+------+\n",
      "|   12|125448|\n",
      "|   11|113254|\n",
      "|   10| 96635|\n",
      "|    9| 91761|\n",
      "|    8| 95835|\n",
      "|    7| 97012|\n",
      "|    6| 95485|\n",
      "|    5| 95251|\n",
      "|    4| 87400|\n",
      "|    3| 89348|\n",
      "|    2| 77779|\n",
      "|    1| 84711|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql((\"\"\"\n",
    "    SELECT Month, COUNT(*) AS Count \n",
    "    FROM nypddatabase.df \n",
    "    GROUP BY Month\n",
    "    ORDER BY Month DESC\n",
    "    \"\"\"))\n",
    "                      \n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4bd29cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_data.write.csv(\"ready_data.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f7908",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfd755e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_data = spark.read.csv(\"ready_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ecbc942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146573"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "606985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, ChiSqSelector\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80ca560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Neighborhood (double): 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Offence_Code (int): 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Offence_Type (string): 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:==============>                                          (2 + 6) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Status (string): 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Offence_Level (string): 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 175:>                                                        (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Borough (string): 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Latitude (double): 96433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Longitude (double): 97259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Lat_Lon (string): 102112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Victim_Sex (string): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Date (date): 1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 211:============================>                            (4 + 4) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Hour (int): 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Year (int): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Month (int): 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of Day_of_Week (string): 7\n",
      "unique number of Victim_Agegroup (string): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 235:=======>                                                 (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "str_featues = []\n",
    "num_features = []\n",
    "\n",
    "for feature in cleaned_data.dtypes:\n",
    "    \n",
    "    # print unique value and data type for each feature\n",
    "    feature_name = str(feature[0])\n",
    "    feature_type = str(feature[1])\n",
    "    uni = cleaned_data.select(feature_name).distinct().count()\n",
    "    print(\"unique number of {} ({}):\".format(feature_name, feature_type),uni)\n",
    "    \n",
    "    # find out features with integer type\n",
    "    if feature_type == \"int\" or feature_type == \"decimal(65,0)\":\n",
    "        \n",
    "        num_features.append(feature_name)\n",
    "        \n",
    "    # find out string feature and it's unique value less than 70\n",
    "    elif feature_type == \"string\" and uni < 70:\n",
    "        \n",
    "        str_featues.append(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861fda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4 numerical features:['Offence_Code', 'Hour', 'Year', 'Month']\n",
      "Total 7 string features:['Offence_Type', 'Status', 'Offence_Level', 'Borough', 'Victim_Sex', 'Day_of_Week', 'Victim_Agegroup']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total {} numerical features:{}\".format(len(num_features),num_features)) \n",
    "print(\"Total {} string features:{}\".format(len(str_featues),str_featues)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a3b6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert categorical string features into numerical indices:\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in str_featues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d83405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to prepare a feature vector for machine learning in PySpark\n",
    "inputs = [\"{}_index\".format(x) for x in str_featues] + num_features\n",
    "feature_vector = VectorAssembler(inputCols= list(set(inputs) - set('Offence_Level_index')), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "044f2f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Then, we create a PySpark ML Pipeline to preprocess the data and apply transformations systematically\n",
    "pipeline = Pipeline(stages=indexers + [feature_vector])\n",
    "data = pipeline.fit(cleaned_data).transform(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a946b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now choose input data \n",
    "input_data = data.select([\"features\",\"Offence_Level_index\"])\n",
    "input_data = input_data.withColumnRenamed(\"Offence_Level_index\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5151de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, we divide our data into 0.7 train, 0.3 test\n",
    "train_data, test_data = input_data.randomSplit([.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6096a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluators for different metrics\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='label', metricName='accuracy')\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol='label', metricName='weightedPrecision')\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol='label', metricName='weightedRecall')\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol='label', metricName='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbe342",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d96c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lg_model = LogisticRegression(labelCol='label', maxIter=10, elasticNetParam=1, regParam=0.5,threshold=0.3)\n",
    "lg_model = lg_model.fit(train_data)\n",
    "lg_pred = lg_model.transform(test_data)\n",
    "acc = accuracy_evaluator.evaluate(lg_pred)\n",
    "prec = precision_evaluator.evaluate(lg_pred)\n",
    "recall = recall_evaluator.evaluate(lg_pred)\n",
    "f1 = recall_evaluator.evaluate(lg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "411dd5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5068581449908248"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7bad817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5068581449908248"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb74f3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25690517914354"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd180b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5068581449908248"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b58103d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(labelCol='label',maxBins=75,numTrees=3,maxDepth=3)\n",
    "rf_model = rf_model.fit(train_data)\n",
    "rf_pred = rf_model.transform(test_data)\n",
    "acc = accuracy_evaluator.evaluate(rf_pred)\n",
    "prec = precision_evaluator.evaluate(rf_pred)\n",
    "recall = recall_evaluator.evaluate(rf_pred)\n",
    "f1 = recall_evaluator.evaluate(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79ba90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9976945947829319\n",
      "Precision: 0.9977049637297467\n",
      "Recall: 0.9976945947829319\n",
      "F1 Score: 0.9976945947829319\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d840fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 359:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 359:==============>                                          (2 + 6) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(labelCol='label', maxDepth=5, maxBins=70)\n",
    "dt_model = dt_model.fit(train_data)\n",
    "dt_pred = dt_model.transform(test_data)\n",
    "acc = accuracy_evaluator.evaluate(dt_pred)\n",
    "prec = precision_evaluator.evaluate(dt_pred)\n",
    "recall = recall_evaluator.evaluate(dt_pred)\n",
    "f1 = recall_evaluator.evaluate(dt_pred)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a8d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
